<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="sharmila">
<meta name="description" content="With the evolution of transfer learning approaches in image processing, the field of Natural Language Processing has also a ubiquitous pre-trained model which is used for multiple states of the art transfer learning solutions for Text classification, Named Entity Recognition.
And this pre-trained model is Word Embeddings. Word embedding is a vector representation of vocabulary which is trained following the concept ‚Äúmeaning of the word is carried by its correspondence‚Äù Excuse me if I have misphrased Ahem!" />
<meta name="keywords" content=", python, nlp, glove, word embeddings" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="#252627" />
<link rel="canonical" href="https://sarmilaupadhyaya.github.io/posts/2019/09/loading-glove-pre-trained-word-embedding-model-from-text-file-faster/" />


    <title>
        
            Loading Glove Pre-trained Word Embedding Model from Text File [Faster] :: ü§ì Sharmila Upadhyaya ü§ì  ‚Äî Hello Friend NG Theme
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="https://sarmilaupadhyaya.github.io/main.dede02da9537a98158079c023e83573e18127834838ef08172acce888341a797.css">




    <link rel="apple-touch-icon" sizes="180x180" href="https://sarmilaupadhyaya.github.io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://sarmilaupadhyaya.github.io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://sarmilaupadhyaya.github.io/favicon-16x16.png">
    <link rel="manifest" href="https://sarmilaupadhyaya.github.io/site.webmanifest">
    <link rel="mask-icon" href="https://sarmilaupadhyaya.github.io/safari-pinned-tab.svg" color="#252627">
    <link rel="shortcut icon" href="https://sarmilaupadhyaya.github.io/favicon.ico">
    <meta name="msapplication-TileColor" content="#252627">
    <meta name="theme-color" content="#252627">



<meta itemprop="name" content="Loading Glove Pre-trained Word Embedding Model from Text File [Faster]">
<meta itemprop="description" content="With the evolution of transfer learning approaches in image processing, the field of Natural Language Processing has also a ubiquitous pre-trained model which is used for multiple states of the art transfer learning solutions for Text classification, Named Entity Recognition.
And this pre-trained model is Word Embeddings. Word embedding is a vector representation of vocabulary which is trained following the concept ‚Äúmeaning of the word is carried by its correspondence‚Äù Excuse me if I have misphrased Ahem!">
<meta itemprop="datePublished" content="2019-09-11T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2019-09-11T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="529">
<meta itemprop="image" content="https://sarmilaupadhyaya.github.io/"/>



<meta itemprop="keywords" content="python,nlp,glove,word embeddings," />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://sarmilaupadhyaya.github.io/"/>

<meta name="twitter:title" content="Loading Glove Pre-trained Word Embedding Model from Text File [Faster]"/>
<meta name="twitter:description" content="With the evolution of transfer learning approaches in image processing, the field of Natural Language Processing has also a ubiquitous pre-trained model which is used for multiple states of the art transfer learning solutions for Text classification, Named Entity Recognition.
And this pre-trained model is Word Embeddings. Word embedding is a vector representation of vocabulary which is trained following the concept ‚Äúmeaning of the word is carried by its correspondence‚Äù Excuse me if I have misphrased Ahem!"/>



    <meta property="og:title" content="Loading Glove Pre-trained Word Embedding Model from Text File [Faster]" />
<meta property="og:description" content="With the evolution of transfer learning approaches in image processing, the field of Natural Language Processing has also a ubiquitous pre-trained model which is used for multiple states of the art transfer learning solutions for Text classification, Named Entity Recognition.
And this pre-trained model is Word Embeddings. Word embedding is a vector representation of vocabulary which is trained following the concept ‚Äúmeaning of the word is carried by its correspondence‚Äù Excuse me if I have misphrased Ahem!" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://sarmilaupadhyaya.github.io/posts/2019/09/loading-glove-pre-trained-word-embedding-model-from-text-file-faster/" />
<meta property="og:image" content="https://sarmilaupadhyaya.github.io/"/>
<meta property="article:published_time" content="2019-09-11T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-09-11T00:00:00+00:00" />






    <meta property="article:published_time" content="2019-09-11 00:00:00 &#43;0000 UTC" />








    </head>

    <body class="dark-theme">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="https://sarmilaupadhyaya.github.io/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text">$ cd /home/</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://sarmilaupadhyaya.github.io/about/">About</a></li><li><a href="https://sarmilaupadhyaya.github.io/mycv/">myCV</a></li><li><a href="https://sarmilaupadhyaya.github.io/posts/">Posts</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>3 minutes

            

            </p>
        </div>

        <article>
            <h1 class="post-title">
                <a href="https://sarmilaupadhyaya.github.io/posts/2019/09/loading-glove-pre-trained-word-embedding-model-from-text-file-faster/">Loading Glove Pre-trained Word Embedding Model from Text File [Faster]</a>
            </h1>

            
                <img src="https://sarmilaupadhyaya.github.io/assets/images/glove-python.jpeg" class="post-cover" />
            

            <div class="post-content">
                <p>With the evolution of transfer learning approaches in image processing, the field of Natural Language Processing has also a ubiquitous pre-trained model which is used for multiple states of the art transfer learning solutions for Text classification, Named Entity Recognition.</p>
<p>And this pre-trained model is <em><strong>Word Embeddings</strong></em>. Word embedding is a vector representation of vocabulary which is trained following the concept ‚Äúmeaning of the word is carried by its correspondence‚Äù Excuse me if I have misphrased Ahem!</p>
<p>[For further elaboration in the State of Transfer Learning in NLP, follow this link which I went through .]</p>
<p>[Additionally, now if you want to know about word embeddings then follow the <a href="https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa">following link</a>.]</p>
<p>Moving forward, we have available pre-trained models like glove, w2vec, fasttext which can be easily loaded and used. In this tutorial, I am just gonna cover how to load .txt file provided by glove in python as a model (which is a dictionary) and getting vector representation of words. These vector representations can be used in other models as input. Usually while loading a text file as a model what we do is, read it line by line and separate word from the vector and insert that word as a key and vector as the value in the dictionary. Well, this takes a long time to load. To optimize this time, what we can do is make two separate files for vocab and vector. Numpy saving array as npy (binary format) file is handy and fast to read as well. Hence, here we will split .txt file into .vocab and .npy file (vector file).</p>
<p>**Step 1: **Download the desired pre-trained embedding file.</p>
<p>Follow the link below and pre-trained word embedding provided by the glove. You can download glove pre-trained model<a href="https://nlp.stanford.edu/projects/glove/"> through this link</a>. I have downloaded 100 dimensions of embedding which was derived from 2B tweets, 27B tokens, 1.2M vocab. The vector length is 100 features.</p>
<p><strong>Step 2:</strong> Now, load the text file into word embedding model in python. Following is the code snippet.
{% gist 2c7c4427248fe89188d36cf6241c53f5%}
<em>Use it as</em> : model = load_glove_model(‚Äúpath/to/txt/file/also/exclude/extension of filename.‚Äù)</p>
<p><strong>Alternative and Faster Way</strong></p>
<p><strong>Step 1:</strong> Once you have a text file, then we will convert it to vocab and npy file.</p>
<p>Now vocab file will have all the words while npy file will have their respective vectors. Consequently, Later they will be mapped. Now, let&rsquo;s see a snippet of code to convert .txt file to the required file.</p>
<p>{% gist d9a80e4ed66a0e1648f48da13edc6406 %}</p>
<p>Use it as: convert_to_binary(‚Äúpath/to/txt/file/also/exclude/extension of filename.‚Äù)</p>
<p>Taaadaa, your desired files are created.</p>
<p>**Step 2: **load these files to get the model.
{% gist 3fc9a979259df954c97a99a3ab4e822c %}
load_embeddings_binary(‚Äúpath/to/vocab and npy/file/also/exclude/extension of filename.‚Äù) Here, our filename of both files are same and extension will be different.</p>
<p>**Step 3: **Get word vector representation. Now, when you pass a sentence, its gonna give a vector of dimension [number of words, dimension of vector]
{% gist 4cb676f0a97346dd6fa3a43ec94409f1 %}
<strong>Example:</strong> get_w2v(‚ÄúThis is demonstration‚Äù, model)</p>
<p>You will get a numpy array of dimension 3 * 100, 3 being the number of tokens and 100 dimension of the feature vector.</p>
<p>This is it! Thank you! Next Session I will be explaining loading embedding model using popular NLP library Gensim.</p>

            </div>
        </article>

        <hr />

        <div class="post-info">
                <p>
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg><span class="tag"><a href="https://sarmilaupadhyaya.github.io/tags/python">python</a></span><span class="tag"><a href="https://sarmilaupadhyaya.github.io/tags/nlp">nlp</a></span><span class="tag"><a href="https://sarmilaupadhyaya.github.io/tags/glove">glove</a></span><span class="tag"><a href="https://sarmilaupadhyaya.github.io/tags/word-embeddings">word embeddings</a></span>
                </p>

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>529 Words</p>

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>2019-09-11 05:45 &#43;0545</p>
        </div>

        
            <div class="pagination">
                <div class="pagination__title">
                    <span class="pagination__title-h"></span>
                    <hr />
                </div>

                <div class="pagination__buttons">
                    
                        <span class="button previous">
                            <a href="https://sarmilaupadhyaya.github.io/posts/2019/10/welcome-to-my-blog/">
                                <span class="button__icon">‚Üê</span>
                                <span class="button__text">Welcome to my blog</span>
                            </a>
                        </span>
                    

                    
                        <span class="button next">
                            <a href="https://sarmilaupadhyaya.github.io/posts/2018/12/sammon-mapping-using-steepest-descent-iterative-approach/">
                                <span class="button__text">Sammon Mapping Using Steepest-Descent Iterative Approach</span>
                                <span class="button__icon">‚Üí</span>
                            </a>
                        </span>
                    
                </div>
            </div>
        
    </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy; 2020</span>
            
                <span><a href="https://sarmilaupadhyaya.github.io/">Jane Doe</a></span>
            
            
                <span><a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0</a></span>
            
            <span> <a href="https://sarmilaupadhyaya.github.io/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a></span>
        </div>
    </div>
    <div class="footer__inner">
        <div class="footer__content">
            <span>Powered by <a href="http://gohugo.io">Hugo</a></span>
            <span>Made with &#10084; by <a href="https://github.com/rhazdon">Djordje Atlialp</a></span>
        </div>
    </div>
</footer>

            
        </div>

        




<script type="text/javascript" src="https://sarmilaupadhyaya.github.io/bundle.min.dc716e9092c9820b77f96da294d0120aeeb189b5bcea9752309ebea27fd53bbe6b13cffb2aca8ecf32525647ceb7001f76091de4199ac5a3caa432c070247f5b.js" integrity="sha512-3HFukJLJggt3&#43;W2ilNASCu6xibW86pdSMJ6&#43;on/VO75rE8/7KsqOzzJSVkfOtwAfdgkd5BmaxaPKpDLAcCR/Ww=="></script>



    </body>
</html>
