<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on ü§ì Sharmila Upadhyaya ü§ì</title>
    <link>https://sarmilaupadhyaya.github.io/post/</link>
    <description>Recent content in Posts on ü§ì Sharmila Upadhyaya ü§ì</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Wed, 06 Nov 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://sarmilaupadhyaya.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Loading Dataset into Google Collaboration using different way</title>
      <link>https://sarmilaupadhyaya.github.io/post/2019-11-06-google-collab-dataset/</link>
      <pubDate>Wed, 06 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://sarmilaupadhyaya.github.io/post/2019-11-06-google-collab-dataset/</guid>
      <description>Google collab provides an ubiquitous platform for all of us, with different hardware selection (CPU, GPU and TPU) band 12 GB of RAM (25 GB if your session crashes. I sometimes willingly crash the collab with some sample of code so that I will get 25 GB of ram for that particular session). At least in the case of training any model, I, personally prefer google collab to build and train my model.</description>
    </item>
    
    <item>
      <title>Building Chatbot using RASA NLU. Part I</title>
      <link>https://sarmilaupadhyaya.github.io/post/2019-10-31-tutorial-documentation/</link>
      <pubDate>Thu, 31 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://sarmilaupadhyaya.github.io/post/2019-10-31-tutorial-documentation/</guid>
      <description>In this tutorial, we will be setting up RASA and making a simple chatbot with it. This chatbot will answer the questions about snacks and acknowledge the user&amp;rsquo;s response. It will be the simplest chatbot ever. This is part one of the rasa series. We will be covering the installation and simple usage of RASA only.
What is RASA?
It is an open-source machine learning framework to create virtual assistants/chatbots. It uses NLP pipelines and architecture for a chatbot and provides excellent flow.</description>
    </item>
    
    <item>
      <title>Welcome to my blog</title>
      <link>https://sarmilaupadhyaya.github.io/post/2017-07-27-welcome/</link>
      <pubDate>Tue, 22 Oct 2019 17:00:00 +0000</pubDate>
      
      <guid>https://sarmilaupadhyaya.github.io/post/2017-07-27-welcome/</guid>
      <description>Hey! Welcome to my blog. I am Sharmila Upadhyaya, an Natural Language Processing Engineer. I am enthusiast machine learning explorer and trying to fit in the tech world. Besides tech and all mind boggling amazing NLP problems, I find time to sing and write poems. I occassonally travel too and will try to include my experience here. :)</description>
    </item>
    
    <item>
      <title>Loading Glove Pre-trained Word Embedding Model from Text File [Faster]</title>
      <link>https://sarmilaupadhyaya.github.io/post/2019-09-11-loading-glove-pre-trained-word-embedding-model-from-text-file-faster/</link>
      <pubDate>Wed, 11 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://sarmilaupadhyaya.github.io/post/2019-09-11-loading-glove-pre-trained-word-embedding-model-from-text-file-faster/</guid>
      <description>With the evolution of transfer learning approaches in image processing, the field of Natural Language Processing has also a ubiquitous pre-trained model which is used for multiple states of the art transfer learning solutions for Text classification, Named Entity Recognition.
And this pre-trained model is Word Embeddings. Word embedding is a vector representation of vocabulary which is trained following the concept ‚Äúmeaning of the word is carried by its correspondence‚Äù Excuse me if I have misphrased Ahem!</description>
    </item>
    
    <item>
      <title>Sammon Mapping Using Steepest-Descent Iterative Approach</title>
      <link>https://sarmilaupadhyaya.github.io/post/2018-12-06-sammon-mapping-using-steepest-descent-iterative-approach/</link>
      <pubDate>Thu, 06 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://sarmilaupadhyaya.github.io/post/2018-12-06-sammon-mapping-using-steepest-descent-iterative-approach/</guid>
      <description>Sammon mapping (SM), an algorithm for dimensionality reduction, follows non-linear projection based mapping of N-dimensional data to a lower dimension.
Why projecting?
Projection is one of the major step used during visualization and data analysis. We can easily picture data in 2D as well as 3D but anything higher than these dimensions, it is difficult for us to visualize. In such a case, we need to project whole datasets to a lower dimension (i.</description>
    </item>
    
  </channel>
</rss>